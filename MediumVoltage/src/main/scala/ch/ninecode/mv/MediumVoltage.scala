package ch.ninecode.mv

import java.io.Closeable
import java.nio.charset.StandardCharsets
import java.text.SimpleDateFormat
import java.util.TimeZone

import scala.collection.mutable.HashMap

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.slf4j.Logger
import org.slf4j.LoggerFactory

import ch.ninecode.cim.CIMNetworkTopologyProcessor
import ch.ninecode.cim.CIMRDD
import ch.ninecode.gl.GLMEdge
import ch.ninecode.gl.GridLABD
import ch.ninecode.gl.TransformerSet
import ch.ninecode.gl.Transformers
import ch.ninecode.model.BaseVoltage
import ch.ninecode.model.ConductingEquipment
import ch.ninecode.model.Element
import ch.ninecode.model.PowerTransformerEnd
import ch.ninecode.model.Terminal
import ch.ninecode.model.TopologicalNode

case class MediumVoltage (session: SparkSession, options: MediumVoltageOptions) extends CIMRDD
{
    if (options.verbose)
    {
        org.apache.log4j.LogManager.getLogger ("ch.ninecode.mv.MediumVoltage").setLevel (org.apache.log4j.Level.INFO)
        org.apache.log4j.LogManager.getLogger ("ch.ninecode.mv.Feeder").setLevel (org.apache.log4j.Level.INFO)
        org.apache.log4j.LogManager.getLogger ("ch.ninecode.cim.CIMNetworkTopologyProcessor").setLevel (org.apache.log4j.Level.INFO)
    }
    implicit val spark: SparkSession = session
    implicit val log: Logger = LoggerFactory.getLogger (getClass)

    // for dates without time zones, the timezone of the machine is used:
    //    date +%Z
    // timezone can be set on each node of the cluster with:
    //    dpkg-reconfigure tzdata
    // then choose Europe and then choose Zürich
    //
    // all dates generated by this program include the time zone
    val use_utc = true
    val date_format = new SimpleDateFormat ("yyyy-MM-dd HH:mm:ss z")
    if (use_utc)
        date_format.setTimeZone (TimeZone.getTimeZone ("UTC"))

    def using[T <: Closeable, R](resource: T)(block: T => R): R =
    {
        try { block (resource) }
        finally { resource.close () }
    }

    def run (): Long =
    {
        val start = System.nanoTime ()

        // read the file
        val reader_options = new HashMap[String, String] ()
        reader_options ++= options.cim_reader_options
        reader_options.put ("path", options.files.mkString (","))
        reader_options.put ("ch.ninecode.cim.make_edges", "false")
        reader_options.put ("ch.ninecode.cim.do_join", "false")
        reader_options.put ("ch.ninecode.cim.do_topo", "false")
        reader_options.put ("ch.ninecode.cim.do_topo_islands", "false")
        val elements = session.read.format ("ch.ninecode.cim").options (reader_options).load (options.files:_*)
        log.info (elements.count () + " elements")

        val read = System.nanoTime ()
        log.info ("read: " + (read - start) / 1e9 + " seconds")

        val storage_level = options.cim_reader_options.find (_._1 == "StorageLevel") match
        {
            case Some ((_, storage)) => StorageLevel.fromString (storage)
            case _ => StorageLevel.fromString ("MEMORY_AND_DISK_SER")
        }

        // identify topological nodes if necessary
        val tns = session.sparkContext.getPersistentRDDs.filter(_._2.name == "TopologicalNode")
        if (tns.isEmpty || tns.head._2.isEmpty)
        {
            val ntp = new CIMNetworkTopologyProcessor (session, storage_level, force_retain_switches = true, force_retain_fuses = false)
            val ele = ntp.process (identify_islands = true)
            log.info (ele.count () + " elements")
        }

        val topo = System.nanoTime ()
        log.info ("topology: " + (topo - read) / 1e9 + " seconds")

        val _transformers = new Transformers (session, storage_level)
        val tdata = _transformers.getTransformerData (topological_nodes = true)

        // feeder service area calculations
        val feeder = Feeder (session, options.storage)
        val nodes_feeders = feeder.identifyFeeders.filter (_._2 != null) // (nodeid, feederid)

        // get a map of voltage for each TopologicalNode starting from Terminal elements
        // ToDo: fix these 1kV multiplier on the voltages
        log.info ("creating nodes")
        val voltages = get("BaseVoltage").asInstanceOf[RDD[BaseVoltage]].map(v ⇒ (v.id, v.nominalVoltage * 1000.0)).collectAsMap
        val end_voltages = getOrElse[PowerTransformerEnd].map (
            x ⇒
            {
                val voltage = voltages.getOrElse (x.TransformerEnd.BaseVoltage, x.ratedU * 1000.0)
                (x.TransformerEnd.Terminal, voltage)
            }
        )
        val zeros = end_voltages.filter (_._2 == 0.0)
        if (!zeros.isEmpty ())
            log.warn ("""transformer ends with no nominal voltage, e.g. %s""".format (zeros.take (5).map (_._1).mkString (",")))
        val equipment_voltages = getOrElse[Terminal].keyBy (_.ConductingEquipment).join (getOrElse[ConductingEquipment].keyBy (_.id)).values.map (
            x ⇒
            {
                val voltage = voltages.getOrElse (x._2.BaseVoltage, 0.0)
                (x._1.id, voltage)
            }
        )
        val nodevoltages = end_voltages.filter (_._2 != 0.0).union (equipment_voltages.filter (_._2 != 0))
            .join (getOrElse[Terminal].keyBy (_.id)).values
            .map (x ⇒ (x._2.TopologicalNode, x._1))

        // put it all together
        val ff = nodes_feeders.join (get[TopologicalNode].keyBy (_.id)).leftOuterJoin (nodevoltages).map (x ⇒ (x._1, (x._2._1._1, x._2._1._2, x._2._2))) // (nodeid, (feederid, TopologicalNode, voltage?))
        val nodes: RDD[(String, FeederNode)] = ff.leftOuterJoin (feeder.feederNodes).values // ((feederid, TopologicalNode, voltage?), feeder?)
            .map (x ⇒ (x._1._1, FeederNode.toFeederNode (x._2.map (List(_)).orNull, x._1._2.id, voltages.getOrElse (x._1._2.BaseVoltage, x._1._3.getOrElse (0.0)))))

        // get equipment with nodes & terminals
        log.info ("creating edges")
        val gg: RDD[(String, Iterable[(String, Terminal)])] = get[Terminal].map (x ⇒ (x.ConductingEquipment, (x.TopologicalNode, x))).groupByKey // (equipmentid, [(nodeid, terminal)])
        // eliminate 0Ω links
        val hh = gg.filter (x ⇒ x._2.groupBy (_._1).size > 1)
        val eq: RDD[(Iterable[(String, Terminal)], Element)] = get[ConductingEquipment].keyBy (_.id).join (get[Element]("Elements").keyBy (_.id)).map (x ⇒ (x._1, x._2._2)) // (elementid, Element)
            .join (hh).values.map (_.swap) // ([(nodeid, terminal)], Element)
            // eliminate edges with only one end
            .filter (x ⇒ (x._1.size > 1) && x._1.map (_._1).forall (_ != null)) // ([(nodeid, terminal)], Element)
        // index by feeder
        val jj: RDD[(String, (Iterable[(String, Terminal)], Element))] = eq.flatMap (x ⇒ x._1.map (y ⇒ (y._1, x))).join (nodes_feeders).values.distinct.map (_.swap) // (feederid, ([(nodeid, Terminal)], Element)
        // ToDo: is it better to groupBy feeder first?
        val kk: RDD[Iterable[(String, (Iterable[(String, Terminal)], Element))]] = jj.keyBy (x ⇒ x._2._1.map (_._1).toArray.sortWith (_ < _).mkString ("_")).groupByKey.values // [(feederid, ([(nodeid, Terminal)], Element)]
        // make one edge for each unique feeder it's in
        val ll: RDD[(String, Iterable[(Iterable[(String, Terminal)], Element)])] = kk.flatMap (x ⇒ x.map (_._1).toArray.distinct.map (y ⇒ (y, x.filter (_._1 == y).map (_._2))))

        // make edges
        // ToDo: fix this collect
        val transformers = tdata.groupBy (_.terminal1.TopologicalNode).values.map (_.toArray).map (TransformerSet (_)).collect
        def make_edge (transformers: Array[TransformerSet]) (args: Iterable[(Iterable[(String, Terminal)], Element)]): GLMEdge =
        {
            // the terminals may be different for each element, but their TopologicalNode values are the same, so use the head
            val id_cn_1 = args.head._1.head._2.TopologicalNode
            val id_cn_2 = args.head._1.tail.head._2.TopologicalNode
            AbgangKreis.toGLMEdge (transformers, options.base_temperature) (args.map (_._2), id_cn_1, id_cn_2)
        }
        val edges: RDD[(String, GLMEdge)] = ll.map (x ⇒ (x._1, make_edge (transformers) (x._2))).cache

        // OK, so there are nodes and edges identified by feeder, one (duplicate) node and edge for each feeder
        log.info ("creating models")
        val feeders = nodes.groupByKey.join (edges.groupByKey).join (feeder.feederStations.keyBy (_._4.id))
            .map (x ⇒ (x._1, (x._2._1._1, x._2._1._2, x._2._2))) // (feederid, ([FeederNode], [GLMEdge], (stationid, abgang#, header, feeder))
            .map (x ⇒ FeederArea (x._1, x._2._3._1, x._2._3._2, x._2._3._3, x._2._1.groupBy (_.id).map (y ⇒ y._2.head), x._2._2.groupBy (_.key).map (y ⇒ y._2.head)))
        log.info ("%s feeders".format (feeders.count))

        def generate (gridlabd: GridLABD, area: FeederArea): Int =
        {
            if (options.verbose) // re-set the log level on each worker
                org.apache.log4j.LogManager.getLogger ("ch.ninecode.mv.MediumVoltage").setLevel (org.apache.log4j.Level.INFO)

            val generator = MvGLMGenerator (one_phase = true, temperature = options.temperature, date_format = date_format, area, voltages)
            gridlabd.export (generator)
            val normally_open = "%s,OPEN".format (date_format.format (0L)).getBytes (StandardCharsets.UTF_8)
            val normally_closed = "%s,CLOSED".format (date_format.format (0L)).getBytes (StandardCharsets.UTF_8)
            // for switches on the boundary, we need to force them to be normally closed, so make a list of interior node ids
            val n = area.nodes.map (_.id).toArray
            // add a player file for each switch
            area.edges.filter (_.isInstanceOf[PlayerSwitchEdge]).map (_.asInstanceOf[PlayerSwitchEdge]).foreach (
                edge ⇒
                {
                    val boundary = !n.contains (edge.cn1) || !n.contains (edge.cn2)
                    val state = if (boundary) normally_closed else if (feeder.switchClosed (edge.switch)) normally_closed else normally_open
                    gridlabd.writeInputFile (generator.name + "/input_data", edge.id + ".csv", state)
                }
            )
            log.info ("%10s %8s %s".format (area.feeder, area.station, area.description))
            1
        }
        val gridlabd = new GridLABD (session, topological_nodes = true, one_phase = !options.three, storage_level = storage_level, workdir = options.workdir)
        log.info ("exporting models")
        val count = feeders.map (generate (gridlabd, _)).sum.longValue

        // for filename in STA*; do echo $filename; pushd $filename > /dev/null; gridlabd $filename; popd > /dev/null; done;

        count
    }
}

object MediumVoltage
{
    /**
     * The list of classes that can be persisted.
     */
    lazy val classes: Array[Class[_]] =
    {
        Array (
            classOf[ch.ninecode.mv.AbgangKreis],
            classOf[ch.ninecode.mv.EdgeData],
            classOf[ch.ninecode.mv.Feeder],
            classOf[ch.ninecode.mv.FeederArea],
            classOf[ch.ninecode.mv.FeederNode],
            classOf[ch.ninecode.mv.MediumVoltage],
            classOf[ch.ninecode.mv.MediumVoltageOptions],
            classOf[ch.ninecode.mv.MediumVoltageGLMGenerator],
            classOf[ch.ninecode.mv.MvGLMGenerator],
            classOf[ch.ninecode.mv.PlayerSwitchEdge],
            classOf[ch.ninecode.mv.USTKreis],
            classOf[ch.ninecode.mv.USTNode],
            classOf[ch.ninecode.mv.VertexData]
        )
    }
}