#!/usr/bin/env bash

# not sure about this: source /etc/bash.bashrc

# Set default HDFS user if not set
if [[ -z "${HDFS_USER}" ]]; then
  export HDFS_USER=spark
fi

# from start-hadoop:

# Create user for HDFS
adduser --no-create-home --disabled-password --gecos "" $HDFS_USER
# Fix directory permissions
chown -R $HDFS_USER:$HDFS_USER /opt/hdfs
chown -R $HDFS_USER:$HDFS_USER $HADOOP_HOME

# from start-hadoop-datanode (edited), expects the name node host as a parameter:

if [[ -z "${1}" ]]; then
  echo "Name node host not specified" >&2
  exit 1
fi

# if localhost, start the name node
if [[ "localhost" == "${1}" ]]; then
    export NAMENODE=$(hostname)
    sed -i "s|\[NAMENODE_HOST\]|$NAMENODE|g" $HADOOP_CONF_DIR/core-site.xml
    start-hadoop-namenode &
else
    # just edit the configuration
    export NAMENODE=${1}
    sed -i "s|\[NAMENODE_HOST\]|$NAMENODE|g" $HADOOP_CONF_DIR/core-site.xml
fi

# Wait for the name node to be online
while ! nc -z $NAMENODE 50070; do
  sleep 2;
done;

# configure Cassandra cluster
sed -i "s|cluster_name: 'Test Cluster'|cluster_name: 'sandbox'|g" /etc/cassandra/cassandra.yaml
seed=$(getent ahosts $NAMENODE | grep $NAMENODE | cut --delimiter " " --fields 1) && sed -i "s|- seeds: \"127.0.0.1\"|- seeds: \"$seed\"|g" /etc/cassandra/cassandra.yaml
me=$(hostname -i) && sed -i "s|listen_address: localhost|listen_address: $me|g" /etc/cassandra/cassandra.yaml

# start Cassandra
service cassandra start 2>/dev/null

# load Cassandra schema
until [[ $(nodetool statusbinary 2>/dev/null) == "running" ]]; do
    sleep 1
done
cqlsh --file /opt/util/bin/schema.sql localhost

# if not localhost, add the Spark master setting as the connection URL to the system.properties
if [[ "localhost" == "${1}" ]]; then
    echo "SparkConnectionFactory.ConnectionURL=local[*]" >> /usr/local/tomee/conf/system.properties
else
    echo "SparkConnectionFactory.ConnectionURL=spark://$NAMENODE:7077" >> /usr/local/tomee/conf/system.properties
fi

catalina.sh run