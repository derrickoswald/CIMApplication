#!/usr/bin/env bash

# not sure about this: source /etc/bash.bashrc

# Set default HDFS user if not set
if [[ -z "${HDFS_USER}" ]]; then
  export HDFS_USER=spark
fi

# from start-hadoop:

# Create user for HDFS
adduser --no-create-home --disabled-password --gecos "" $HDFS_USER
# Fix directory permissions
chown -R $HDFS_USER:$HDFS_USER /opt/hdfs
chown -R $HDFS_USER:$HDFS_USER $HADOOP_HOME

# from start-hadoop-datanode (edited), expects the name node host as a parameter:

if [[ -z "${1}" ]]; then
  echo "Name node host not specified" >&2
  exit 1
fi

# if localhost start the name node
if [[ "localhost" == "${1}" ]]; then
    export NAMENODE=$(hostname)
    sed -i "s|\[NAMENODE_HOST\]|$(hostname)|g" $HADOOP_CONF_DIR/core-site.xml
    start-hadoop-namenode &
else
    # just edit the configuration
    export NAMENODE=${1}
    sed -i "s|\[NAMENODE_HOST\]|${1}|g" $HADOOP_CONF_DIR/core-site.xml
fi

# Wait for the name node to be online
while ! nc -z $NAMENODE 50070; do
  sleep 2;
done;

catalina.sh run