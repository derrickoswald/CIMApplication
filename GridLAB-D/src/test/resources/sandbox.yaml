# Spark local cluster using Docker
#
# Note: from GridLAB-D/src/test/resources/ directory run with:
#       docker-compose up&
# stop with:
#       docker-compose down
#
# Note: for more worker nodes use:
#       docker-compose scale worker=2
#
# Note: to start a shell on the cluster use:
#       docker exec --interactive --tty spark_master bash
# or    docker exec --interactive --tty spark_worker_1 bash
#

version: "2" # Docker Engine (version > 1.10.0) and Docker Compose (version >= 1.6.0)

services:
  sandbox:
    container_name: spark_master
    image: derrickoswald/spark-docker
    command: start-spark master
    environment:
      HDFS_USER: ${USER} # use value of environment variable "USER" as Spark owner/operator 
    hostname: sandbox
    ports: # not sure how many of these are really needed:
      - "4040:4040"
      - "6066:6066"
      - "7070:7070"
      - "7077:7077" # Spark standalone port, as in master = spark://sandbox:7077
      - "8020:8020" # hdfs port, as in: hdfs dfs -fs hdfs://sandbox:8020 -ls
      - "8032:8032"
      - "8042:8042"
      - "8081:8080"
      - "8088:8088"
      - "10000:10000" # Thriftserver JDBC port
      - "10001:10001" # Thriftserver HTTP protocol JDBC port
      - "10004:10004" # CIMServerJDBC default JDBC port
      - "50010:50010"
      - "50070:50070"
    volumes:
      - ./../../../target:/opt/code # map compilation output directory to /opt/code within the container
  worker:
    image: derrickoswald/spark-docker
    command: start-spark worker sandbox
    environment:
      HDFS_USER: ${USER} # use value of environment variable "USER" as Spark owner/operator
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 4g
    links:
      - sandbox
    volumes:
      - ./../../../target:/opt/code # map compilation output directory to /opt/code within the container
